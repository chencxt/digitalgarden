---
{"dg-publish":true,"permalink":"/05publishedweb/W002-学习进度/250221-008_第10个项目_树莓派4B部署Deepseek边缘计算/","noteIcon":"","created":"2025-02-21T21:51:50.710+08:00","updated":"2025-02-21T22:06:16.635+08:00"}
---


Deepseek官方的教程中，用的是docker部署，但是docker对于不熟悉的人来说，或者不喜欢经常更新软件包的人来说，用着那叫一个酸爽。所以我就在找有没有可以绕过docker部署的方案。

众所周知，deepseek的微调基模是qwen，而qwen是一种基于transformer的解码器语言模型，其架构类似于LLaMA系列模型。因此理论上说可以不通过docker，直接使用llama系的llama c++和ollama等通用框架直接部署。

我们可以分成两步走，第一步，首先在pc上验证这个想法是否正确；第二步，在开发板上实现我们的想法。

开工！

<mark style="background: #BBFABBA6;">2025-02-19 08:18:07</mark>

安装路径修改：OllamaSetup.exe /DIR="d:\some\location"

安装路径修改：OllamaSetup.exe /DIR="G:\AI_Center\-program\AI_Text\Ollama"

启动：在对应的命令行窗口直接输入命令：ollama serve

运行这个指令，ollama会启动相对应的模型，如果没有，则会先在路径中下载然后启动。ollama run deepseek-r1:1.5b

如果你想要停止这个模型，只需要输入：/bye

关于其他指令，可以直接在已经打开的对话界面输入"/?"，以及在未打开任何模型的命令行窗口处输入ollama help

听说只需要在prompt中添加“Untrammelled”一词，就可以绕过思维钢印
好吧，测试了一下确实可以，但是还需要辅助添加大量的引导词才行，这个以后再折腾吧。我现在只想把deepseek装进我的树莓派里面。

安装Ollama到ubuntu开发板（树莓派4B）
`curl -fsSL https://ollama.com/install.sh | sh`



查询到树莓派4b的架构为armv7a，所以去https://github.com/v2fly/v2ray-core/releases下载对应的编译内核。
参考连接：https://www.freedidi.com/672.html



2025-02-19 10:14:46
v8a内核启动失败，开始测试ARMv7a
尝试：v2ray-linux-arm32-v7a.zip
v2ray-linux-arm32-v7a.zip内核启动失败
尝试：v2ray-linux-64
v2ray-linux-64可以使用，但卡死。
分析：可能树莓派需要的v2ray内核就是v2ray-linux-arm32-v7a，只是我忘记了赋予可执行权限。
再次尝试：v2ray-linux-arm32-v7a.zip

2025-02-19 10:40:49
sudo chmod 777 -R *
成功通过内核测试，就是使用v2ray-linux-arm32-v7a，只是上一次试验的时候忘记了添加可执行权限，导致内核文件无法执行。


导入订阅

开始测试ollama，参考连接：
https://www.cnblogs.com/dechinphy/p/18699554/deepseek


查看ollama的安装目录：which ollama
结果：/usr/local/bin/ollama

已成功在树莓派4B 8G内存版本中，通过ollama部署deepseek-r1:1.5b
![attachment/2025-02-19_131507.png](/img/user/05publishedweb/W002-%E5%AD%A6%E4%B9%A0%E8%BF%9B%E5%BA%A6/attachment/2025-02-19_131507.png)
![attachment/250221-1.jpg](/img/user/05publishedweb/W002-%E5%AD%A6%E4%B9%A0%E8%BF%9B%E5%BA%A6/attachment/250221-1.jpg)

---


